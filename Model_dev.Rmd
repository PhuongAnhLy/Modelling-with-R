---
title: "Model_Dev"
output:
  html_document: default
  pdf_document: default
date: "2023-01-14"
---

## Load tidyverse package

```{r}
library(tidyverse)
```

## Load the dataset

```{r}
# url where the data is located
url <- "https://dax-cdn.cdn.appdomain.cloud/dax-airline/1.0.1/lax_to_jfk.tar.gz"

# download the file
download.file(url, destfile = "lax_to_jfk.tar.gz")

# if you run this on your local machine, then can remove tar = "internal"
untar("lax_to_jfk.tar.gz")

# read_csv only
sub_airline <- read_csv("lax_to_jfk/lax_to_jfk.csv",
                        col_types = cols('DivDistance' = col_number(),
                                         'DivArrDelay' = col_number()))
```

## 1. Simple Linear Regression
### 1.1. Define dataset with just AA as the Reporting_Airline

```{r}
aa_delays <- sub_airline %>%
  filter(CarrierDelay != "NA", Reporting_Airline == "AA")
head(aa_delays)
```
### 1.2. How departure delay (DepDelayMinutes) can help us predict arrival delay (ArrDelayMinutes)?

We will create a linear function with "DepDelayMinutes" as the predictor variable and the "ArrDelayMinutes" as the response variable.

```{r}
linear_model <- lm(ArrDelayMinutes ~ DepDelayMinutes, data = aa_delays)
```

### 1.3. Summarize the regression model using summary()

```{r}
summary(linear_model)
```
### 1.4. Input data we use to predict

```{r}
new_depdelay <- data.frame(
  DepDelayMinutes = c(12, 19, 24))
```

### 1.5. Predict the data points

The "fit" column is the prediction results of the inputs and "lwr" and "upr" are the lower bound and upper bound of the 95% confidence intervals of prediction results.

```{r}
pred <- predict(linear_model, newdata = new_depdelay, interval = "confidence")
pred
```
### 1.6. Using the fitted model, linear_model, you can grab the attribute coefficients using $. 

These coefficients correspond to 𝑏0 (the intercept) and 𝑏1 (the slope and coefficient of DepDelayMinutes)

```{r}
linear_model$coefficients
```
## 2. Multiple Linear Regression
### 2.1. Develop a model using 2 predictor variables by fitting the data

```{r}
mlr <- lm(ArrDelayMinutes ~ DepDelayMinutes + LateAircraftDelay, data = aa_delays)
summary(mlr)

mlr$coefficients
```
### 2.2. Develop a model using 3 predictor variables by fitting the data

```{r}
mlr2 <- lm(ArrDelayMinutes ~ DepDelayMinutes + 
             LateAircraftDelay + CarrierDelay, data = aa_delays)
summary(mlr)
mlr2$coefficients
```
### 2.3. Using mlr2, what are the predicted values for the following new data points?

```{r}
# New data points
DepDelayMinutes <- c(10, 20, 30)
LateAircraftDelay <- c(20, 60, 30)
new_multidelay <- data.frame(DepDelayMinutes, LateAircraftDelay)

# predicted values
pred <- predict(mlr, 
                newdata = new_multidelay,
                interval = "confidence")
pred
```
## 3. Assessing Models Visually
### 3.1. Regression Plot

```{r}
ggplot(aa_delays, aes(x = DepDelayMinutes, y = ArrDelayMinutes)) +
  geom_point() + 
  stat_smooth(method = "lm", col = "red")
```
### 3.2. Residual Plot

```{r}
aa_delays <- sub_airline %>%
  filter(CarrierDelay != "NA", Reporting_Airline == "AA")
score_model <- lm(ArrDelayMinutes ~ DepDelayMinutes, data = aa_delays)
aa_delays$predicted <- predict(score_model)

ggplot(aa_delays, aes(x = DepDelayMinutes, y = ArrDelayMinutes)) +
  geom_smooth(method = "lm", se = FALSE, color = "red") +  # Plot regression slope
  geom_segment(aes(xend = DepDelayMinutes, yend = predicted), alpha = .2) +  # alpha to fade lines
  geom_point() +
  geom_point(aes(y = predicted), shape = 1) +
  theme_bw()  # Add theme for cleaner look
```
### Other Residual Plot

```{r}
ggplot(lm(ArrDelayMinutes ~ DepDelayMinutes, data = aa_delays)) +
  geom_point(aes(x=DepDelayMinutes, y=.resid))
```
### 3.3. Other Diagnostic Plots

```{r}
linear_model <- lm(ArrDelayMinutes ~ DepDelayMinutes, data = aa_delays)
plot(linear_model)
```
## 4. Polynomial Regression

### 4.1. Set the seed

```{r}
set.seed(20)
x <- seq(from=0, to=20, by=0.1)

# value to predict (y):
y <- 500 + 0.4 * (x-10)^3

# some noise is generated and added to the real signal (y):
noise <- rnorm(length(x), mean=10, sd=80)
noisy.y <- y + noise
```

```{r}
# fit linear model
ggplot(data=NULL,aes(x, noisy.y)) + 
    geom_point() + 
    geom_smooth(method = "lm")
```
```{r}
ggplot(data=NULL,aes(x, noisy.y)) + 
    geom_point() + 
    geom_smooth(method = "lm", formula = y ~ poly(x, 5))
```
### 4.2. Polynomial 2nd Order

```{r}
time <- 6:19
temp <- c(4,6,7,9,10,11,11.5,12,12,11.5,11,10,9,8)

ggplot(data = NULL, aes(time, temp)) + 
    geom_point()
```
```{r}
polyfit2 <- lm(temp ~ poly(time, 2, raw = TRUE))

summary(polyfit2)
```
```{r}
ggplot(data = NULL, aes(time, temp)) + 
    geom_point() + 
    geom_smooth(method = "lm", formula = y ~ poly(x, 2)) 
```


## 5. Assessing the Model

```
Two very important measures that are often used in Statistics to determine the accuracy of a model are:
- R^2 / R-squared
- Mean Squared Error (MSE)
```

### 5.1. Model 1: Simple Linear Regression

```{r}
linear_model <- lm(ArrDelayMinutes ~ DepDelayMinutes, aa_delays)
```

```{r}
mse <- mean(linear_model$residuals^2)
mse
```
```{r}
rmse <- sqrt(mse)
rmse
```
```{r}
summary(linear_model)$r.squared
```
### 5.2. Model 2: Multiple Linear Regression

```{r}
mlr <- lm(ArrDelayMinutes ~ DepDelayMinutes + LateAircraftDelay, data = aa_delays)
```

```{r}
mse_mlr <- mean(mlr$residuals^2)
mse_mlr
```
```{r}
rmse_mlr <- sqrt(mse_mlr)
rmse_mlr
```
```{r}
summary(mlr)$r.squared
```
### 5.3. Model 3: Polynomial Regression

```{r}
poly_reg <- lm(ArrDelayMinutes ~ poly(DepDelayMinutes, 3), data = aa_delays)
```

```{r}
mse_poly <- mean(poly_reg$residuals^2)
mse_poly
```
```{r}
rmse_poly <- sqrt(mse)
rmse_poly
```
```{r}
summary(poly_reg)$r.squared
```
## 6. Prediction and Decision Making

```{r}
head(predict(score_model))
```
```
- What is a good R-squared value?
  When comparing models, the model with the higher R-squared value is a better fit for the data.
- What is a good MSE?
  When comparing models, the model with the smallest MSE value is a better fit for the data.
  
Conclusion: 
Comparing these three models, the MLR model performs slightly better than the SLR model. Perhaps if we tried adding some more predictor variables, the MLR model could do even better. Of the three models, we conclude that the polynomial of order 3 model seems to be the best fit it as it has the highest R^2 and the lowest MSE. 

```



