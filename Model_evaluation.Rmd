---
title: "Model_Evaluation"
output: html_document
date: "2023-01-15"
---

```{r}
# Library for modeling
library(tidymodels)

# Load tidyverse
library(tidyverse)
```

```{r}
# url where the data is located
url <- "https://dax-cdn.cdn.appdomain.cloud/dax-airline/1.0.1/lax_to_jfk.tar.gz"

# download the file
download.file(url, destfile = "lax_to_jfk.tar.gz")

# untar the file so we can get the csv only
# if you run this on your local machine, then can remove tar = "internal" 
untar("lax_to_jfk.tar.gz", tar = "internal")

# read_csv only 
sub_airline <- read_csv("lax_to_jfk/lax_to_jfk.csv",
                     col_types = cols('DivDistance' = col_number(), 
                                      'DivArrDelay' = col_number()))
```

## 1. Model Evaluation
### 1.1 Training and Testing Data

An important step in testing your model is to split your data into training and testing data. The training data will be used to train (fit) models, while the testing data will not be touched until we are evaluating the model.

Using other packages or programming languages may require to separate out the reponse variable (`ArrDelayMinutes` in this case) into another dataframe, but here that is not necessary. The response and predictor variables can all stay in one dataframe.

Before splitting the data we:

*   Use the principles learned in module 2 and use `replace_na()` to replace the NAs in the variables we are using to predict. Here, we choose to replace the values with 0 because having NA in these variables mean that there was no delay.
*   Use `select()` to only include the variables we will use to create a final model.

```{r}
flight_delays <- sub_airline %>% 
    replace_na(list(CarrierDelay = 0,
                    WeatherDelay = 0,
                    NASDelay = 0,
                    SecurityDelay = 0,
                    LateAircraftDelay = 0)) %>%
    select(c(ArrDelayMinutes, DepDelayMinutes, CarrierDelay, WeatherDelay, NASDelay, SecurityDelay, LateAircraftDelay, DayOfWeek, Month))
```

Now, with the prepared dataset flight_delays, you can split the data. A random seed is set so that the way the data is split will be the same every time this code is run, this helps create reproducible results.

```{r}
set.seed(1234)
flight_split <- initial_split(flight_delays)
train_data <- training(flight_split)
test_data <- testing(flight_split)
```

In initial_split(), you can also set the prop parameter to set the proportion of the data to use for training. If it is unspecified like here in the example, then by default it is set to 0.75. This means that the proportion of data that is split into the training data is 75% (so the testing data is 25%).

Use the function `initial_split()` to split up the data set such that 80% of the data samples will be utilized for training: 

```{r}
flight_split2 <- initial_split(flight_delays, prop = 0.8)
train_data2 <- training(flight_split2)
test_data2 <- testing(flight_split2)
```

### 1.2 Training a Model

After splitting the dataset, the next step is to create a Linear Regression object by using `linear_reg()` to specify linear regression and `set_engine()` to specify which package is used to create the model.

```{r}
# Pick linear regression
lm_spec <- linear_reg() %>%
  # Set engine
  set_engine(engine = "lm")

# Print the linear function
lm_spec
```
In this example, we will use Arrival Delay Minutes ("ArrDelayMinutes") as the response variable and Departure Delay Minutes ("DepDelayMinutes") as the predictor variable to fit (train) a model. We will use train_data because we are training the model. The test_data will be used later.

Use `fit()` to fit the model we just specified in `lm_spec`. The output is the fitted (trained) model.

```{r}
train_fit <- lm_spec %>% 
    fit(ArrDelayMinutes ~ DepDelayMinutes, data = train_data)

train_fit 
```
To look at some of the predictions of the fitted model, use `predict()`, which will output one column with predictions (`.pred`). Here, since `new_data = train_data`, you are looking at how well the model is predicting the original training data.

```{r}
train_results <- train_fit %>%
  # Make the predictions and save the predicted values
  predict(new_data = train_data) %>%
  # Create a new column to save the true values
  mutate(truth = train_data$ArrDelayMinutes)

head(train_results)
```
Now it is time to evaluate the models to estimate how well the models perform on new data, the test data. This example uses the same model in train_fit to make the predictions. 

Again, from `predict()`, the output is stored in a data frame with only one column, called `.pred`. You can then add a new column to this data frame using the `mutate()` function. This new column is named truth and contains values of "ArrDelayMinutes" from the `test_data`. 

In the end, you will have a dataframe with the predictions and the true values.

```{r}
test_results <- train_fit %>%
  # Make the predictions and save the predicted values
  predict(new_data = test_data) %>%
  # Create a new column to save the true values
  mutate(truth = test_data$ArrDelayMinutes)

head(test_results)
```
### 1.3 Evaluating the Model

a. Calculate RMSE

```{r}
rmse(train_results, truth = truth,
     estimate = .pred)

rmse(test_results, truth = truth,
     estimate = .pred)
```
b. Calculate R-squared

```{r}
rsq(train_results, truth = truth,
    estimate = .pred)

rsq(test_results, truth = truth,
    estimate = .pred)
```
c. Visualize how well you predictedÂ the Arrival Delay Minutes

Let's break down the code:

1.  `mutate` - add column called `train` to test_results and set the values all to "testing"
2.  `bind_rows` - do the same to the train_results and bind these rows the test_results
3.  `ggplot` - plot the truth vs prediction values
4.  `geom_abline` - add the y=x line
5.  `geom_point` - add the truth vs prediction points to the plot
6.  `facet_wrap` - since `train` contains two values "testing" and "training", this splits the data into two graphs
7.  `labs` - add labels

```{r}
test_results %>%
  mutate(train = "testing") %>%
  bind_rows(train_results %>% mutate(train = "training")) %>%
  ggplot(aes(truth, .pred)) +
  geom_abline(lty = 2, color = "orange", 
              size = 1.5) +
  geom_point(color = '#006EA1', 
             alpha = 0.5) +
  facet_wrap(~train) +
  labs(x = "Truth", 
       y = "Predicted Arrival Delays (min)")
```

### 1.4 Cross validation

To perform cross validation, you can use `vfold_cv()`. Setting v = 10 means that it will use 10 folds. The function `fit_resamples()` will keep refitting the model specified on the samples specified by the cross validation object.

```{r}
set.seed(1234)
cv_folds <- vfold_cv(train_data, v = 10)
results <- fit_resamples(lm_spec, 
                         ArrDelayMinutes ~ DepDelayMinutes,
                         resamples = cv_folds)
```

```{r}
results %>% collect_metrics()
```
## 2. Overfitting, Underfitting and Model Selection

a. An example of underfitting:

```{r}
ggplot(cars, aes(x = speed, y = dist)) + 
    geom_point() + 
    geom_hline(yintercept = mean(cars$dist), 
               col = "red") 
```

b. An example of overfitting:

```{r}
ggplot(cars, aes(x = speed, y = dist)) + 
    geom_point() + 
    geom_smooth(method = "lm", 
                formula = y ~ poly(x, 8), 
                col = "red", se = FALSE) 
```

You can reduce the complexity of the model. In the previous overfitting example, a polynomial model of 8 degrees was used. Instead, you can use a polynomial of degree 1 or a simple linear regression model. In R, you can set the formula to **y over x**. In this example, we demonstrated how you can prevent overfitting and underfitting models by changing the model complexity.

```{r}
ggplot(cars, aes(x = speed, y = dist)) + 
    geom_point() + 
    geom_smooth(method = "lm", 
                formula = y ~ x, 
                col = "red", 
                se = FALSE) 
```

## 3. Regularization

Regularization is a way of **avoiding overfitting** by restricting the magnitude of model coefficients.

### 3.1 Ridge (L2) regularization

a. First, create a `recipe()` that includes the model formula. You could preprocess the data more in this step, but the data here is already preprocessed. The **dot .** in the formula is a special character that tells R to use all the variables in train_data.

```{r}
flight_recipe <-
  recipe(ArrDelayMinutes ~ ., data = train_data)
```

b. Next, use the `linear_reg()` function from the tidymodels library to specify the model.

**penalty** is the value of lambda. **mixture** is the proportion of L1 penalty. For ridge regression, specify mixture = 0. This means there is no L1 penalty and only the L2 penalty is used. For **lasso regression**, you would use mixture = 1.

```{r}
ridge_spec <- linear_reg(penalty = 0.1, mixture = 0) %>%
  set_engine("glmnet")
```

c. Next, create a workflow object so you can more conveniently combine pre-processing, modeling, and post-processing requests.

```{r}
ridge_wf <- workflow() %>%
  add_recipe(flight_recipe)
```

d. Finally, add the ridge model and fit the model.

```{r}
library(glmnet)
ridge_fit <- ridge_wf %>%
  add_model(ridge_spec) %>%
  fit(data = train_data)
```
e. View the result of the fitted ridge regression model, use the `pull_workflow_fit()` function.

```{r}
ridge_fit %>%
  pull_workflow_fit() %>%
  tidy()
```
### Comparing Regularization Types

Now that you know more about regularization, it is also good to understand when you would use a techinque over the other.

*   **Lasso (L1)**:
    *   Pros: Lasso is primarily used for variable selection, that is, reducing the number of variables/features used in a model by shrinking the coefficients to zero. You would use this if you have many variables and think just a select few would will be useful in a final model.
    *   Cons: The downside of Lasso is that its variable selection is unstable, as in, for correlated variables it will arbitrarily select one. Additionally, if the number of data point (n) is less than the number of features (p), then Lasso can select at most n of the features.

*   **Ridge (L2)**:
    *   Pros: If you donât want to reduce the number of variables, you can use this. Ridge also works well when there is multicollinearity in the features because it reduces the variance while increasing bias.
    *   Cons: Will not reduce the number of variables if that is your goal. Also, the bias in the model may be high.

*   **Elastic net (L1/L2)**:
    *   Pros: Elastic net combines the benefits of Lasso and Ridge. It solves some of the issues that Lasso has when doing variable selection because it works well when the variables are highly correlated and it can work when the number of variables is greater than the number of samples.
    *   Cons: May be computationally more expensive than Lasso or Ridge because it computes both L1 and L2 penalties.


